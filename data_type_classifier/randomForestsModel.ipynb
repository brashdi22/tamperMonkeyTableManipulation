{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0efc5f73-50e1-4288-8031-d98affd98b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff05ef4-201c-4745-9f0e-d4f3019cd28d",
   "metadata": {},
   "source": [
    "## Data preperation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "664a2a0a-7644-4b5a-a5a1-09b6d8116c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data.csv', header=None)\n",
    "df = df.T\n",
    "df.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f436213e-8b00-4a1b-b455-5196c6d87776",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_names = {\n",
    "    'nominal': 1,\n",
    "    'ordinal': 2,\n",
    "    'numerical': 3,\n",
    "    'textual': 4,\n",
    "    'date': 5, \n",
    "    'unknown': 0\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e124fc0-bf02-4ea9-9e33-fe58e2bdb0c3",
   "metadata": {},
   "source": [
    "##### Tried finding common words in the title, tried including the ratio of special characters as a feature, but these did not affect the performance at all. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee92ce70-beee-4d66-8f03-ec30e950ad57",
   "metadata": {},
   "source": [
    "##### Reducing the data classes from 5 to 4 (by combining 'date' and 'textual') does not affect the accuarcy of the 'ordinal', 'nominal', and 'numerical' classes. I combined them as I say there is no added benefit in keeping them separate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "2bb58de6-bf41-408b-87fb-2bbb074cc3b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_data_type_from_title(title):\n",
    "    # Convert title to lower case for case-insensitive matching\n",
    "    title_lower = title.lower()\n",
    "\n",
    "    # Keywords associated with different data types\n",
    "    categorical_keywords = ['type', 'category', 'class', 'group', 'status', 'id', 'code', 'tag', 'gender', 'color', 'brand',\n",
    "                            'name', 'city', 'country', 'level', 'grade', 'rank', 'stage', 'rating', 'score', 'priority', \n",
    "                            'severity', 'size', 'quality']\n",
    "    \n",
    "    numerical_keywords = ['number', 'amount', 'rate', 'price', 'cost', 'value', 'volume', 'length', 'weight', 'height', \n",
    "                          'width', 'depth', 'temperature', 'salary', 'count', 'percentage', 'score', 'distance', 'age',\n",
    "                          'hour', 'minute', 'second', 'period', 'duration']\n",
    "    \n",
    "    textual_keywords = ['description', 'notes', 'comments', 'text', 'address', 'summary', 'content', 'remarks', \n",
    "                        'message', 'date', 'time', 'year', 'month', 'day', 'hour', 'minute', 'second', 'timestamp', \n",
    "                        'period', 'duration', 'deadline']\n",
    "\n",
    "    # Initialize one-hot encoded list\n",
    "    one_hot = [0, 0, 0]\n",
    "\n",
    "    # Check for presence of keywords in the title and set the corresponding index to 1\n",
    "    if any(keyword in title_lower for keyword in categorical_keywords):\n",
    "        one_hot[0] = 1\n",
    "    if any(keyword in title_lower for keyword in numerical_keywords):\n",
    "        one_hot[1] = 1\n",
    "    if any(keyword in title_lower for keyword in textual_keywords):\n",
    "        one_hot[2] = 1\n",
    "\n",
    "    return one_hot\n",
    "\n",
    "def count_delimiters(data_cells):\n",
    "    delimiters = ['/', '-', ':']\n",
    "    delimiters_ratio = []\n",
    "    \n",
    "    for delimiter in delimiters:\n",
    "        delimiters_ratio.append(sum(cell.count(delimiter) for cell in data_cells.astype(str)) / len(data_cells))\n",
    "\n",
    "    return delimiters_ratio\n",
    "\n",
    "# # Initialize counts for special characters\n",
    "# special_chars = {'%': 0, '-': 0, '#': 0, '/': 0, '$': 0, '£': 0, '€': 0}\n",
    "\n",
    "# # Count occurrences of each special character\n",
    "# for cell in data_cells:\n",
    "#     for char in special_chars:\n",
    "#         if char in str(cell):\n",
    "#             special_chars[char] += 1\n",
    "\n",
    "# # Calculate the ratio of counts to the total number of cells\n",
    "# total_cells = len(data_cells)\n",
    "# special_char_ratios = [count / total_cells for count in special_chars.values()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "d1621497-ac76-4ff3-a683-a5338dc12e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_number_ratio(cell):\n",
    "    \"\"\"\n",
    "    Calculate the ratio of text characters to number characters in a cell.\n",
    "    \"\"\"\n",
    "    text_chars = sum(c.isalpha() for c in str(cell))\n",
    "    number_chars = sum(c.isdigit() for c in str(cell))\n",
    "    total_chars = text_chars + number_chars\n",
    "    return text_chars / number_chars if number_chars > 0 else 0\n",
    "\n",
    "def analyze_column(column):\n",
    "    \"\"\"\n",
    "    Analyzes a single column and returns various statistics based on the content, with improvements.features\n",
    "\n",
    "    :param column: Pandas Series representing a column from a DataFrame.\n",
    "    :return: Dictionary containing various statistics about the column.\n",
    "    \"\"\"\n",
    "\n",
    "    title = column.iloc[0]\n",
    "    # label = column.iloc[-1]\n",
    "    data_cells = column.iloc[1:-1]\n",
    "\n",
    "    \n",
    "    # Apply the ratio calculation to each cell and then find the average\n",
    "    avg_text_to_number_ratio = data_cells.apply(text_to_number_ratio).mean()\n",
    "\n",
    "    # Ratio of number of unique values to total number of values\n",
    "    unique_to_total_ratio = data_cells.nunique() / len(data_cells)\n",
    "\n",
    "    # Convert data_cells to numeric, coerce errors, and then drop NaNs for numeric calculations\n",
    "    numeric_column = pd.to_numeric(data_cells, errors='coerce').dropna()\n",
    "\n",
    "    # Calculate the percentage of non-NaN values\n",
    "    percentage_non_nan = len(numeric_column) / len(data_cells) * 100\n",
    "\n",
    "    # Min, Max, Mean, Median, Std for numeric data\n",
    "    if percentage_non_nan >= 80:\n",
    "        min_val = numeric_column.min()\n",
    "        max_val = numeric_column.max()\n",
    "        mean_val = numeric_column.mean()\n",
    "        median_val = numeric_column.median()\n",
    "        std_val = numeric_column.std()\n",
    "        min_str_length = max_str_length = mean_str_length = median_str_length = std_str_length = 0\n",
    "        avg_dashes = 0\n",
    "        avg_slashes = 0\n",
    "        \n",
    "    else:  # Non-numeric data\n",
    "        min_val = max_val = mean_val = median_val = std_val = 0\n",
    "        str_lengths = data_cells.apply(lambda x: len(str(x)))\n",
    "        min_str_length = str_lengths.min()\n",
    "        max_str_length = str_lengths.max()\n",
    "        mean_str_length = str_lengths.mean()\n",
    "        median_str_length = str_lengths.median()\n",
    "        std_str_length = str_lengths.std()\n",
    "    \n",
    "    return [\n",
    "        avg_text_to_number_ratio, unique_to_total_ratio,\n",
    "        min_val, max_val, mean_val, median_val, std_val,\n",
    "        min_str_length, max_str_length, mean_str_length, median_str_length, std_str_length\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "cee2bf93-d237-43c2-b3c5-224233a7f4a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the feature engineering function to each row\n",
    "feature_matrix = df.apply(analyze_column, axis=1)\n",
    "\n",
    "# Convert the resulting series of lists/arrays into a matrix\n",
    "feature_matrix = np.array(feature_matrix.tolist())\n",
    "labels = df.iloc[:, -1]\n",
    "labels = labels.str.replace('date', 'textual', case=False, regex=True)\n",
    "# Split the data into training-testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(feature_matrix, labels, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "b3953570-8495-400b-abf7-d18345c98753",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12,)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_matrix[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f03b571-83b5-47f7-a9b0-4cb462c1966e",
   "metadata": {},
   "source": [
    "## Random forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "61352c56-6999-4fe8-a787-7d2572ffb868",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      " Categorical       0.75      0.82      0.78        11\n",
      "   Numerical       0.92      1.00      0.96        11\n",
      "     Textual       0.86      0.67      0.75         9\n",
      "\n",
      "    accuracy                           0.84        31\n",
      "   macro avg       0.84      0.83      0.83        31\n",
      "weighted avg       0.84      0.84      0.83        31\n",
      "\n",
      "Accuracy: 0.8387096774193549\n"
     ]
    }
   ],
   "source": [
    "# Initialize the Random Forest Classifier\n",
    "rf_classifier = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Train the model\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = rf_classifier.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "141f2a95-ccbe-4af7-8268-ad14073a1f5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 576 candidates, totalling 1728 fits\n",
      "Best parameters: {'bootstrap': True, 'max_depth': None, 'max_features': 'log2', 'min_samples_leaf': 4, 'min_samples_split': 10, 'n_estimators': 100}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  Categorial       0.89      0.73      0.80        11\n",
      "   Numerical       0.79      1.00      0.88        11\n",
      "     Textual       0.88      0.78      0.82         9\n",
      "\n",
      "    accuracy                           0.84        31\n",
      "   macro avg       0.85      0.84      0.83        31\n",
      "weighted avg       0.85      0.84      0.84        31\n",
      "\n",
      "Accuracy: 0.8387096774193549\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300, 350],  # Number of trees in the forest\n",
    "    'max_depth': [None, 10, 20, 30],  # Maximum depth of the tree\n",
    "    'min_samples_split': [2, 5, 10],  # Minimum number of samples required to split an internal node\n",
    "    'min_samples_leaf': [1, 2, 4],    # Minimum number of samples required to be at a leaf node\n",
    "    'max_features': ['log2', 'sqrt'], # Number of features to consider when looking for the best split\n",
    "    'bootstrap': [True, False]        # Whether bootstrap samples are used when building trees\n",
    "}\n",
    "\n",
    "\n",
    "# Initialize the Grid Search model\n",
    "grid_search = GridSearchCV(estimator=rf_classifier, param_grid=param_grid, cv=3, n_jobs=-1, verbose=2)\n",
    "\n",
    "# Fit the grid search to the data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best parameters\n",
    "print(\"Best parameters:\", grid_search.best_params_)\n",
    "best_model = grid_search.best_estimator_\n",
    "# Predict on the test set\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae06d06-07a9-4c92-b394-670aaa542f84",
   "metadata": {},
   "source": [
    "#### To save the model as ONNX model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "245abe8f-c7dc-40ba-93a0-551713ab9881",
   "metadata": {},
   "outputs": [],
   "source": [
    "import skl2onnx\n",
    "from skl2onnx import convert_sklearn\n",
    "from skl2onnx.common.data_types import FloatTensorType\n",
    "import onnxruntime as ort\n",
    "import joblib\n",
    "\n",
    "# 'model' is the trained scikit-learn model and 'X_test', 'y_test' are the test sets\n",
    "# Convert to ONNX\n",
    "initial_type = [('float_input', FloatTensorType([None, X_test.shape[1]]))]\n",
    "onnx_model = convert_sklearn(rf_classifier, initial_types=initial_type)\n",
    "\n",
    "# Save the ONNX model to a file\n",
    "onnx_model_path = 'model.onnx'\n",
    "with open(onnx_model_path, \"wb\") as f:\n",
    "    f.write(onnx_model.SerializeToString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6d0590b6-780b-4130-9e11-8b5c29c49dc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ONNX Model Accuracy: 0.8387096774193549\n"
     ]
    }
   ],
   "source": [
    "# Test the performance of the ONNX model\n",
    "# Load the ONNX model with ONNX Runtime\n",
    "sess = ort.InferenceSession(onnx_model_path)\n",
    "\n",
    "# Prepare the input data\n",
    "input_data = {sess.get_inputs()[0].name: X_test.astype(np.float32)}\n",
    "\n",
    "# Run inference\n",
    "onnx_output = sess.run(None, input_data)\n",
    "\n",
    "onnx_predictions = onnx_output[0]\n",
    "\n",
    "# Evaluate accuracy\n",
    "onnx_accuracy = accuracy_score(y_test, onnx_predictions)\n",
    "print(\"ONNX Model Accuracy:\", onnx_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0028fb8-ffc1-4148-8201-26c5ba7db8aa",
   "metadata": {},
   "source": [
    "## Gradient Boosting Machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2937ca4e-aed5-4cd4-bcc0-c31ff39cbeb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 6 candidates, totalling 60 fits\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=10, estimator=GradientBoostingClassifier(), n_jobs=-1,\n",
       "             param_grid={&#x27;learning_rate&#x27;: [0.01], &#x27;max_depth&#x27;: [4],\n",
       "                         &#x27;n_estimators&#x27;: [280, 300, 320, 340, 360, 380]},\n",
       "             scoring=&#x27;accuracy&#x27;, verbose=2)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=10, estimator=GradientBoostingClassifier(), n_jobs=-1,\n",
       "             param_grid={&#x27;learning_rate&#x27;: [0.01], &#x27;max_depth&#x27;: [4],\n",
       "                         &#x27;n_estimators&#x27;: [280, 300, 320, 340, 360, 380]},\n",
       "             scoring=&#x27;accuracy&#x27;, verbose=2)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: GradientBoostingClassifier</label><div class=\"sk-toggleable__content\"><pre>GradientBoostingClassifier()</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GradientBoostingClassifier</label><div class=\"sk-toggleable__content\"><pre>GradientBoostingClassifier()</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(cv=10, estimator=GradientBoostingClassifier(), n_jobs=-1,\n",
       "             param_grid={'learning_rate': [0.01], 'max_depth': [4],\n",
       "                         'n_estimators': [280, 300, 320, 340, 360, 380]},\n",
       "             scoring='accuracy', verbose=2)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [280, 300, 320, 340, 360, 380],\n",
    "    'learning_rate': [0.01],\n",
    "    'max_depth': [4],\n",
    "}\n",
    "\n",
    "gbm = GradientBoostingClassifier()\n",
    "\n",
    "grid_search = GridSearchCV(estimator=gbm, param_grid=param_grid, cv=10, scoring='accuracy', n_jobs=-1, verbose=2)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# gbm.fit(X_train, y_train)\n",
    "# y_pred = gbm.predict(X_test)\n",
    "\n",
    "# print(classification_report(y_test, y_pred))\n",
    "# print(\"Accuracy:\", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c0e700f9-8835-4f5d-9cf0-f143e1c5fce7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'learning_rate': 0.01, 'max_depth': 4, 'n_estimators': 300}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  Categorial       0.77      0.91      0.83        11\n",
      "   Numerical       0.85      1.00      0.92        11\n",
      "     Textual       0.80      0.44      0.57         9\n",
      "\n",
      "    accuracy                           0.81        31\n",
      "   macro avg       0.81      0.78      0.77        31\n",
      "weighted avg       0.81      0.81      0.79        31\n",
      "\n",
      "Accuracy: 0.8064516129032258\n"
     ]
    }
   ],
   "source": [
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "y_pred = best_model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb16ec2d-e15e-4db1-819d-066bc64e7876",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0a2deb27-739d-4802-b64c-35e8a3b7b71d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "  Categorial       0.82      0.82      0.82        11\n",
      "   Numerical       0.73      1.00      0.85        11\n",
      "     Textual       0.80      0.44      0.57         9\n",
      "\n",
      "    accuracy                           0.77        31\n",
      "   macro avg       0.78      0.75      0.75        31\n",
      "weighted avg       0.78      0.77      0.76        31\n",
      "\n",
      "Accuracy: 0.7741935483870968\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "model = SVC(kernel='rbf')  # 'rbf' kernel is usually a good choice; can also try 'linear', 'poly'\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "df34a22e-1de6-430c-9c5b-33d0dc61563d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape: (121, 12)\n",
      "y_train.shape: (121,)\n",
      "X_test.shape: (31, 12)\n",
      "y_test.shape: (31,)\n"
     ]
    }
   ],
   "source": [
    "print(\"X_train.shape:\", X_train.shape)\n",
    "print(\"y_train.shape:\", y_train.shape)\n",
    "print(\"X_test.shape:\", X_test.shape)\n",
    "print(\"y_test.shape:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad3ccaf-3646-4997-937d-6690b95c0692",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8fde5528-3263-4566-b406-3ca6353f5759",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Python311\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Python311\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Python311\\Lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "Epoch 1/4\n",
      "WARNING:tensorflow:From C:\\Python311\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Python311\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "4/4 [==============================] - 4s 12ms/step - loss: 0.6821 - accuracy: 0.3884\n",
      "Epoch 2/4\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 0.6279 - accuracy: 0.4711\n",
      "Epoch 3/4\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 0.5596 - accuracy: 0.4711\n",
      "Epoch 4/4\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 0.4995 - accuracy: 0.4711\n",
      "1/1 [==============================] - 1s 851ms/step - loss: 0.3564 - accuracy: 0.3548\n",
      "Accuracy: 0.35483869910240173\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "max_sequence_length = 12\n",
    "\n",
    "# Pad sequences\n",
    "X_train_padded = pad_sequences(X_train, maxlen=max_sequence_length)\n",
    "X_test_padded = pad_sequences(X_test, maxlen=max_sequence_length)\n",
    "\n",
    "# Reshape for LSTM input\n",
    "X_train_padded = X_train_padded.reshape((X_train_padded.shape[0], max_sequence_length, 1))\n",
    "X_test_padded = X_test_padded.reshape((X_test_padded.shape[0], max_sequence_length, 1))\n",
    "\n",
    "# Initialize the encoder\n",
    "encoder = LabelEncoder()\n",
    "# Fit the encoder and transform the labels to numeric\n",
    "y_train_numeric = encoder.fit_transform(y_train)\n",
    "y_test_numeric = encoder.transform(y_test)\n",
    "\n",
    "# Define model\n",
    "model = Sequential()\n",
    "model.add(LSTM(50, return_sequences=True, input_shape=(max_sequence_length, 1)))\n",
    "model.add(LSTM(50, return_sequences=False))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train model\n",
    "model.fit(X_train_padded, y_train_numeric, epochs=4, batch_size=32)\n",
    "\n",
    "# Evaluate model\n",
    "loss, accuracy = model.evaluate(X_test_padded, y_test_numeric)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d658e13-2c2e-4e85-85dd-35f4e49dfd52",
   "metadata": {},
   "source": [
    "## KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8b8bb2ec-158e-4b87-94cd-d3be50e7a02d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8064516129032258\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "X = feature_matrix  # Features\n",
    "y = labels  # Labels\n",
    "\n",
    "# Encode the labels if they are categorical\n",
    "label_encoder = LabelEncoder()\n",
    "y_train = label_encoder.fit_transform(y_train)\n",
    "y_test = label_encoder.fit_transform(y_test)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.fit_transform(X_test)\n",
    "\n",
    "# Create KNN classifier\n",
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "\n",
    "# Train the classifier\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the classifier\n",
    "accuracy = knn.score(X_test, y_test)\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3cc8b20-3661-4e11-b7e7-558d93343678",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
